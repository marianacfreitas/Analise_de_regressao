---
title: "Trabalho de Regressão II"
author: "Mariana Costa freitas"
date: '2024-06-06'
output:
  pdf_document: default
  html_document:
    df_print: paged
---
# Introdução

O diabetes é uma condição crônica que afeta milhões de pessoas em todo o mundo, e seu diagnóstico precoce é fundamental para a prevenção de complicações graves. Nesse contexto, o Instituto Nacional de Diabetes e Doenças Digestivas e Renais realizou um estudo com mulheres Pimas adultas residentes na região de Phoenix, Arizona, com o objetivo de investigar fatores que possam influenciar a concentração de glicose no sangue, medida através de um teste oral de tolerância à glicose.

Este relatório apresenta uma análise de regressão para a variável Glucose, a fim de identificar os principais fatores associados a suas variações. As variáveis utilizadas no estudo incluem características fisiológicas e dados demográficos das participantes:

O objetivo principal deste estudo é construir um modelo de regressão linear para a variável Glucose e verificar a adequação desse modelo através da análise dos pressupostos fundamentais da regressão linear, tais como linearidade, independência dos resíduos, homocedasticidade e normalidade dos resíduos.

Ao final, espera-se identificar as variáveis que mais influenciam a concentração de glicose no sangue e fornecer insights valiosos para estratégias de prevenção e tratamento do diabetes entre as mulheres Pimas.

# Análise dos dados

Nessa análise estamos utilizando os dados disponibilizados pelo Instituto Nacional de Diabetes e Doenças Digestivas e Renais, obtidos por meio de uma pesquisa feita em índias Pimas adultas que vivem perto de Phoenix-Arizona. As variáveis abordadas no banco de dados são:

+ pregnancies: número de gestações
+ glucose: concentração de glicose em teste oral de tolerância à glicose
+ blood_pressure: pressão arterial diastólica (mm Hg)
+ skin_thickness: espessura cutânea triciptal (mm)
+ insulin: 2 horas de insulina no soro (mu U/ml)
+ bmi: índice de massa corporal (IMC)
+ diabetes_pedigree_function: função que mede as chances de ter diabetes baseada no histórico familiar
+ age: idade em anos
+ outcome: resultado do teste para diabetes, que pode ser saudável (outcome=0) ou diabético (outcome=1)

# Análise descritiva

Para melhor compreensão dos dados que estamos utilizando, vamos obter algumas de suas medidas descritivas:

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("GGally")     # grafico em matriz
library("MASS")		    # stepwise (aic)
library("mixlm")		  # stepwise (valor-p)
library("tidyverse")	# manipulacao de dados
library("rvest")
library("reshape2")
library("dplyr")
library("ggplot2")
library("readr")
library("janitor")
library("plotly")
library(car)    	  # vif - multicolinearidade
library(nortest)  	# normalidade
library(lmtest)		  # homocedasticidade e auto-correlação
library(gamlss)		  # incorporando heterocedasticidade
library(nlme)		    # incorporando auto-correlação


# IMPORTANDO DADOS

df <- read_csv("glicose.csv") |>
  clean_names()

library("knitr")
library("papeR")
kable(papeR::summarize(df, type="numeric",  test = FALSE))
```

Essas variáveis apresentam muitos dados faltantes que podem prejudicar nossas análises e posteriormente também o desenvolvimento do modelo de regressão. Assim, é necessário remover od dados faltantes, e remover aquelas variáveis que apresentam alta quantidade desses dados.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
freq_na <- c(nrow(filter(df, glucose == 0)), 
             nrow(filter(df, blood_pressure == 0)),
             nrow(filter(df, skin_thickness == 0)),
             nrow(filter(df, insulin == 0)),
             nrow(filter(df, bmi==0)),
             nrow(filter(df, age==0)),
             nrow(filter(df, is.na(pregnancies))),
             nrow(filter(df, is.na(outcome))),
             nrow(filter(df, diabetes_pedigree_function==0))
             )
nomes <- c("glucose", "blood_pressure", "skin_thickness", "insulin",
           "bmi", "age", "pregnancies", "outcome", "diabetes_pedigree_function")

df_na <- data.frame(nomes, freq_na)
colnames(df_na) <- c("Variável", "Frequência de dados faltantes")
kable(df_na)
# knitr::kable(tabela_freq_na, caption = "Tabela de Frequência")
```
Dessa forma, vamos excluir as variáveis `insulin` e `skin_thickness`e remover os dados faltantes de `glucose`, `blood_pressure` e `bmi`. O restante das variáveis não apresentam dados faltantes, então não vamos alterá-las. Abaixo podemos observar as novas medidas descritivas após o tratamento desses dados.

```{r, warning=FALSE, message=F, echo=FALSE}

# Retirando os dados faltantes

df <- df |>
  select(-c(insulin, skin_thickness)) |>
  filter(glucose != 0 & blood_pressure != 0 & bmi != 0)

kable(papeR::summarize(df, type="numeric",  test = FALSE))


```
Um importante passo na análise descritiva é verificar o comportamento da variável `glucose`, que será a variável descrita pelo modelo de regressão. Abaixo, construímos um histograma para melhor entender a sua distribuição.

```{r, warning=F, message=FALSE, echo=FALSE}
# Histograma da variável 'glucose'

ggplot(df, aes(x = glucose)) +
  geom_density(alpha = 0.5, fill = "violet", col = "violet") +  
  labs(
    #title = "Histograma de Densidade",
    x = "Captura por unidade de pesca",
    y = "Densidade"
  ) + theme_minimal() + 
  geom_vline(xintercept = mean(df$glucose), linetype = "dashed", color = "black")

```

Aqui percebemos que seu gráfico de densidade apresenta leve assimetria a direita, então futuramente talvez seja necessário realizar uma transformação de Box-Cox.

Também vamos observar o comportamento de `glucose` associada a outras variáveis abaixo.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# Gráfico de dispersão glucose x blood_pressure

ggplot(data = df, aes(x = glucose, y = blood_pressure)) +
  geom_point(col = "violet") +  
  labs(x = "Concentração de glicose", y = "Pressão arterial", title = " ")  + theme_minimal()

```

É possível perceber que o comportamento da variável *glucose* não é influenciado pela variável *blood_pressure* e vice-versa, já que não observamos nenhum padrão na distribuição de uma conforme aumento ou diminuição da outra.


```{r, warning=FALSE, message=FALSE, echo=FALSE}

# Box plot glucose por idade discretizada

df2 <- df
df2 <- df2|>
  mutate(
    age_interval = case_when(
      age <= 29 & age >= 21 ~ "21 a 29",
      age <= 35 & age >= 30 ~ "30 a 35",
      age <= 43 & age >= 36 ~ "36 a 43",
      age <= 50 & age >= 44 ~ "44 a 50",
      age <= 56 & age >= 51 ~ "51 a 56",
      age <= 63 & age >= 57 ~ "57 a 63",
      age <= 70 & age >= 64 ~ "64 a 70",
      age <= 75 & age >= 71 ~ "71 a 75",
      age <= 81 & age >= 76 ~ "76 a 81",
    ),
    
    preg_interval = case_when(
      pregnancies == 0 ~ "Sem gestações",
      pregnancies >= 1 & pregnancies <= 2 ~ "1 a 2",
      pregnancies >= 3 & pregnancies <= 5 ~ "3 a 5",
      pregnancies >= 10 & pregnancies <= 13 ~ "10 a 13",
      pregnancies >= 6 & pregnancies <= 9 ~ "6 a 9",
      pregnancies >= 14 & pregnancies <= 17 ~ "14 a 17"
      )
  )

ggplot(df2, aes(x = age_interval, y = glucose, fill = age_interval)) +
  geom_boxplot() +
  labs(
    x = "Idade em anos",
    y = "Concentração de glicose"
  ) + theme_minimal()


```

No boxplot acima, discretizamos a variável que representa idade, por meio de um agrupamento em intervalos. Observamos que, a medida que a idade aumenta, a média da concentração de glicose também aumenta gradualmente, além de apresentar variabilidade levemente maior.


```{r, message=F, warning=FALSE, echo=FALSE}

# Gráfico de dispersão glucose x bmi

ggplot(data = df, aes(x = glucose, y = bmi)) +
  geom_point(col = "violet") +  
  labs(x = "Concentração de glicose", y = "Índice de massa corporal", title = " ")  + theme_minimal()

```

Na representação acima, podemos observar que não há uma tendência a aumento ou diminuição de uma das variáveis enquanto a outra aumenta, o IMC se mantém distribuído da mesma forma conforme a variação da concentração de glicose.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# Gráfico de violino glucose por número de gestações discretizada

df2$preg_interval <- factor(df2$preg_interval, levels = c("Sem gestações", "1 a 2", "3 a 5", "6 a 9", "10 a 13", "14 a 17"))


ggplot(df2, aes(x = preg_interval, y = glucose, fill = preg_interval)) +
  geom_violin() +
  labs(
    title = " ",
    x = "Número de gestações",
    y = "Concentração de glicose",
    fill = 'Número de gestações'
  ) +
  theme_minimal()

```

Nesse gráfico, as idades gestacionais (em semanas) foram agrupadas em intervalos. A partir do gráfico de violino, é possível notar que nos primeiros grupos a glicose se concentra em níveis mais baixos, já a partir das semanas 6 a 9 o nível de glicose começa a se distribuir mais uniformemente, enquanto no último grupo se concentra em níveis de glicose mais altos.


```{r, warning=FALSE, message=FALSE, echo=FALSE}

# Gráfico de dispersão glucose x diabates_pedigree_function

ggplot(data = df, aes(x = glucose, y = diabetes_pedigree_function)) +
  geom_point(col = "violet") +  
  labs(x = "Concentração de glicose", y = "Diabetes função da genealogia", title = " ")  + theme_minimal()

```

Aqui podemos notar que aparentemente não há uma linha reta ou curva que descreva a relação entre as variáveis. Os pontos não seguem uma inclinação positiva (indicando correlação positiva) ou negativa (indicando correlação negativa).

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# Gráfico densidade glucose para os diabéticos e não diabéticos

ggplot(df, aes(x = glucose, fill = as.factor(outcome))) +
  geom_density(alpha = 0.3) +
  labs(
    title = " ",
    x = "Concentração de glicose",
    y = "Teste de diabetes",
    fill = 'Teste de diabetes'
  ) + theme_minimal() + 
  scale_fill_manual(values = c("0" = "violet", "1" = "grey"),
                     labels = c("Saudável", "Diabético"))

```

Acima temos as curvas de densidade para o nível de glicose, segregado em saudável e diabético. A partir do gráfico de densidade podemos observar que há grande diferença nos níveis de glicose entre os dois grupos, tendo os diabéticos grande concentração nos níveis mais altos e os saudáveis conecentrados em torno de 100.

# Análise de correlação

A correlação descreve como as mudanças em uma variável estão associadas às mudanças em outra variável. A correlação é expressa por um coeficiente de correlação, que varia de -1 a 1. Um coeficiente de correlação próximo de 1 indica uma forte correlação positiva, o que significa que as duas variáveis tendem a aumentar juntas. Um coeficiente de correlação próximo de -1 indica uma forte correlação negativa, onde uma variável tende a diminuir quando a outra aumenta. Um coeficiente de correlação próximo de 0 indica que não há correlação linear entre as variáveis.

A correlação é importante na análise de regressão porque ajuda a entender a relação entre as variáveis independentes e a variável dependente. Antes de construir um modelo de regressão, é crucial examinar a correlação entre as variáveis independentes e a variável dependente. Se houver uma correlação forte entre uma variável independente e a variável dependente, isso sugere que a variável independente pode ser um bom preditor da variável dependente e pode ser incluída no modelo de regressão.

A seguir é possível visualizar a correlação entre as variáveis que estamos trabalhando:


```{r, message=FALSE, warning=FALSE, echo=FALSE}

ggcorr(df, geom = "blank", label = TRUE, hjust = 0.75) +
  geom_point(size = 10, aes(color = coefficient >= 0, alpha = abs(coefficient) >= 0.05)) +
  scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
  guides(color = FALSE, alpha = FALSE)

```

Pode-se obervar que, em geral, as correlações entre as variáveis são baixas e todas positivas, sendo as mais correlacionadas oa pares *outcome* e *glucose*, *age* e *pregnancies* e o restante com medidas entre 0 e 0.3. Como nenhuma correlação, é, em módulo, maior que 0.9, não precisamos remover nenhuma covariável.

# Análise de Regressão

## Modelo

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
# MODELO 

modelo <- stats::lm(glucose ~ ., data=df)
summary(modelo)

opt_model_step_aic<- stepAIC(modelo, direction="both") 
summary(opt_model_step_aic)



```

Um modelo de regressão linear visa descrever a relação entre uma variável dependente (também chamada de variável de resposta) e uma ou mais variáveis independentes (também conhecidas como preditoras ou explicativas). Para selecionar o modelo que melhor se ajusta aos dados, vamos usar como base o Critério de Informação de Akaike (AIC), que é uma medida que apresenta menor valor para o melhor modelo. Logo, nosso objetivo é encontrar o modelo que apresenta o menor AIC.

Para isso, vamos utilizar o método de seleção de variáveis chamado *Stepwise*, que começa com o modelo completo, ou seja, com todas as variáveis no modelo e remove ou adiciona as variáveis caso uma dessas opções gere uma diminuição no AIC. Quando nenhuma dessas ações ocasiona um menor AIC, consideramos que o melhor modelo foi obtido. 

Em R, executamos esse processo primeiro utilizando a função `lm()`, que ajusta o modelo aos dados, estimando os coeficientes que melhor ajustam os dados observados. Em seguida, usamos a função,  `stepAIC()`, com argumento `direction="both"`, que funciona seguindo um procedimento iterativo que envolve adicionar ou remover variáveis independentes do modelo, uma de cada vez, e comparar os valores do AIC para determinar se a adição ou remoção da variável resulta em uma melhoria no ajuste do modelo. O processo continua até que nenhuma alteração adicional resulte em uma redução significativa no AIC.

Ao executar esse processo aplicado aos dados do Instituto Nacional de Diabetes e Doenças Digestivas e Renais, obtemos que as variáveis presentes no modelo para descrever a variável *glucose* são *pregnancies*, *bllod_pressure*, *bmi*, *age* e *outcome*, com coeficientes -0.69186, 0.23415, 0.29128, 0.44003 e 27.73519, respectivamente.


## Verificando pressupostos do modelo

Os pressupostos de um modelo de regressão são condições que devem ser atendidas para garantir a validade e a confiabilidade das inferências e previsões feitas pelo modelo. Esses pressupostos são fundamentais para assegurar que as estimativas dos coeficientes do modelo sejam precisas e que os testes estatísticos sejam válidos. Aqui vamos verificar os pontos influentes e de alavanca, a normalidade dos erros e a homocedasticidade. Garantir que esses pressupostos sejam atendidos é essencial para a construção de modelos de regressão robustos e confiáveis, que possam ser usados para tomar decisões informadas e baseadas em dados.

## Multicolinearidade

O pressuposto da multicolinearidade refere-se à situação em que duas ou mais variáveis independentes em um modelo de regressão estão altamente correlacionadas entre si. Em outras palavras, existe uma relação linear quase perfeita entre algumas das variáveis independentes. Esse fenômeno pode ser prejudicial para a interpretação do modelo e para a precisão das estimativas dos coeficientes de regressão. Quando existe multicolinearidade, torna-se difícil interpretar os efeitos individuais de cada variável independente sobre a variável dependente.

Já vimos pelo gráfico de correlação, que não há nenhum par de variáveis altamente correlacionadas, agora vamos verificar, usando a VIF, uma métrica que quantifica o grau de multicolinearidade entre as variáveis independentes. Valores de VIF maiores que 10 são frequentemente considerados indicativos de multicolinearidade significativa. Usando a função `vif()`, do R, observamos que os valores variam de 1.187740 a 1.591839, logo não há multicolinearidade.


```{r, echo=FALSE, warning=FALSE, message=FALSE}

vif(opt_model_step_aic)

```


#### Identificação de Outliers

Valores discrepantes presentes nos dados podem interferir na correta estimação do modelo, assim, é necessário removê-los. Para identificar esses pontos atípicos, utilizamos os resíduos estudentizados, que são uma medida padronizada dos resíduos de um modelo estatístico calculada dividindo o resíduo pelo seu desvio padrão estimado. Essa padronização permite que os resíduos sejam comparados entre si e com valores de referência, facilitando a identificação de observações incomuns ou atípicas no conjunto de dados. Aqui vamos definir os "limites" para esses resíduos estudentizados como 2 e -2, retirando os dados com resíduos fora do intervalo $[-2, 2]$. A seguir representamos esses resíduos graficamente.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

fit<- opt_model_step_aic

n<- nrow(df)    		        # número de observações
k<- length(fit$coef) 		    # k=p+1 (número de coeficientes)

corte.hii<- 2*k/n		        # corte para elementos da diagonal de H
corte.cook<- qf(0.5,k,n-k)	# corte para Distância de Cook
corte.stu<- 2			          # corte para resíduos estudentizados

rst<- rstudent(fit)		      # resíduos estudentizados
hii<- hatvalues(fit) 		    # valores da diagonal da matriz H
dcook<- cooks.distance(fit)	# distância de Cook

obs<- 1:n

df.fit<- data.frame(obs,rst,hii,dcook)

# GRÁFICO - RESÍDUOS ESTUDENTIZADOS

df.fit %>% ggplot(aes(x=obs,y=rst)) + 
  geom_point() + 
  geom_hline(yintercept = c(-corte.stu, corte.stu), color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Resíduo Estudentizado") + 
  theme_bw()

```

Aqui identificamos as observações discrepantes e vamos retirá-las, reconstruindo o modelo e agora obtendo o seguinte gráfico de resíduos estudentizados:

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}

#Retirando os dados discrepantes

obs_index_rst <- df.fit |> filter(rst <= 2 & rst >= -2)
df <- df[obs_index_rst$obs,]

# Refazendo o modelo

modelo2 <- stats::lm(glucose ~ ., data=df)
summary(modelo2)

opt_model_step_aic2 <- stepAIC(modelo2, direction="both") 
summary(opt_model_step_aic2)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

#Refazendo o gráfico dos resíduos estudentizados

fit2 <- opt_model_step_aic2

n2<- nrow(df)    		        # número de observações
k2<- length(fit2$coef) 		    # k=p+1 (número de coeficientes)

corte.hii2<- 2*k2/n2		        # corte para elementos da diagonal de H
corte.cook2<- qf(0.5,k2,n2-k2)	# corte para Distância de Cook
corte.stu2<- 2			          # corte para resíduos estudentizados

rst2<- rstudent(fit2)		      # resíduos estudentizados
hii2<- hatvalues(fit2) 		    # valores da diagonal da matriz H
dcook2<- cooks.distance(fit2)	# distância de Cook

obs2<- 1:n2
df.fit2<- data.frame(obs2,rst2,hii2,dcook2)

df.fit2 %>% ggplot(aes(x=obs2,y=rst2)) + 
  geom_point() + 
  geom_hline(yintercept = c(-corte.stu2, corte.stu2), color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Resíduo Estudentizado") + 
  theme_bw()

```

É possível notar que ainda há a presença de alguns *outliers*, assim, vamos executar o processo de remover as observações e construir o modelo novamente, obtendo:

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}

#Retirando os dados discrepantes

obs_index_rst2 <- df.fit2 |> filter(rst2 <= 2 & rst2 >= -2)
df <- df[obs_index_rst2$obs,]

# Refazendo o modelo

modelo3 <- stats::lm(glucose ~ ., data=df)
summary(modelo3)

opt_model_step_aic3 <- stepAIC(modelo3, direction="both") 
summary(opt_model_step_aic3)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

#Refazendo o gráfico dos resíduos estudentizados

fit3 <- opt_model_step_aic3

n3<- nrow(df)    		        # número de observações
k3<- length(fit3$coef) 		    # k=p+1 (número de coeficientes)

corte.hii3<- 2*k3/n3		        # corte para elementos da diagonal de H
corte.cook3<- qf(0.5,k3,n3-k3)	# corte para Distância de Cook
corte.stu3<- 2			          # corte para resíduos estudentizados

rst3<- rstudent(fit3)		      # resíduos estudentizados
hii3<- hatvalues(fit3) 		    # valores da diagonal da matriz H
dcook3<- cooks.distance(fit3)	# distância de Cook

obs3 <- 1:n3
df.fit3<- data.frame(obs3,rst3,hii3,dcook3)

df.fit3 %>% ggplot(aes(x=obs3,y=rst3)) + 
  geom_point() + 
  geom_hline(yintercept = c(-corte.stu3, corte.stu3), color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Resíduo Estudentizado") + 
  theme_bw()

```

Acima podemos notar que há poucos outliers e que esses estão localizados bem próximos aos "limites" que adotamos. Assim, podemos seguir verificando os demais pressupostos do modelo.

## Pontos de alavanca

Pontos de alavanca são observações que possuem valores extremos nas variáveis independentes (preditoras). Esses pontos têm o potencial de influenciar significativamente a posição da linha de regressão, devido à sua distância em relação à média das variáveis independentes.

Para identificar os pontos de alavanca vamos analisar se o valor de alavanca (calculado a partir da matriz H) de cada observação excede $\frac{2k+1}{n}$, em que $k$ é o número de covariáveis e $n$ o número de observações. 

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# GRÁFICO - ALAVANCAGEM

df.fit3 %>% ggplot(aes(x=obs3,y=hii3,ymin=0,ymax=hii3)) + 
  geom_point() + 
  geom_linerange() + 
  geom_hline(yintercept = corte.hii3, color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Alavancagem") + 
  theme_bw()

```

Aqui percebemos que muitas observações ultrapassam o valor limite estipulado, assim removemos esses dados e reconstruímos o modelo, obtendo:

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}

#Retirando os dados discrepantes

obs_index_hii3 <- df.fit3 |> filter(hii3 <= corte.hii3)
df <- df[obs_index_hii3$obs,]

# Refazendo o modelo

modelo4 <- stats::lm(glucose ~ ., data=df)
summary(modelo4)

opt_model_step_aic4 <- stepAIC(modelo4, direction="both") 
summary(opt_model_step_aic4)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

#Refazendo o gráfico de alavancagem

fit4 <- opt_model_step_aic4

n4<- nrow(df)    		        # número de observações
k4<- length(fit4$coef) 		    # k=p+1 (número de coeficientes)

corte.hii4<- 2*k4/n4		        # corte para elementos da diagonal de H
corte.cook3<- qf(0.5,k4,n4-k4)	# corte para Distância de Cook
corte.stu3<- 2			          # corte para resíduos estudentizados

rst4<- rstudent(fit4)		      # resíduos estudentizados
hii4<- hatvalues(fit4) 		    # valores da diagonal da matriz H
dcook4<- cooks.distance(fit4)	# distância de Cook

obs4 <- 1:n4
df.fit4<- data.frame(obs4,rst4,hii4,dcook4)

df.fit4 %>% ggplot(aes(x=obs4,y=hii4,ymin=0,ymax=hii4)) + 
  geom_point() + 
  geom_linerange() + 
  geom_hline(yintercept = corte.hii4, color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Alavancagem") + 
  theme_bw()



```

Como ainda há valores que ultrpassam o valor de corte, vamos executar esse processo novamente:


```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}

#Retirando os dados discrepantes

obs_index_hii4 <- df.fit4 |> filter(hii4 <= corte.hii4)
df <- df[obs_index_hii4$obs,]

# Refazendo o modelo

modelo5 <- stats::lm(glucose ~ ., data=df)
summary(modelo5)

opt_model_step_aic5 <- stepAIC(modelo5, direction="both") 
summary(opt_model_step_aic5)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

#Refazendo o gráfico de alavancagem

fit5 <- opt_model_step_aic5

n5<- nrow(df)    		        # número de observações
k5<- length(fit5$coef) 		    # k=p+1 (número de coeficientes)

corte.hii5<- 2*k5/n5		        # corte para elementos da diagonal de H
corte.cook5<- qf(0.5,k5,n5-k5)	# corte para Distância de Cook
corte.stu5<- 2			          # corte para resíduos estudentizados

rst5<- rstudent(fit5)		      # resíduos estudentizados
hii5<- hatvalues(fit5) 		    # valores da diagonal da matriz H
dcook5<- cooks.distance(fit5)	# distância de Cook

obs5 <- 1:n5
df.fit5<- data.frame(obs5,rst5,hii5,dcook5)

df.fit5 %>% ggplot(aes(x=obs5,y=hii5,ymin=0, ymax=hii5)) + 
  geom_point() + 
  geom_linerange() + 
  geom_hline(yintercept = corte.hii5, color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Alavancagem") + 
  theme_bw()



```

Aqui há menos observações acima do valor de corte e que ultrapassam esse limite em menor valor. Assim, vamos seguir para os demais pressupostos.

## Pontos influentes

Pontos influentes são observações que têm um impacto significativo nos parâmetros estimados do modelo, podendo distorcer os resultados da regressão e levar a conclusões incorretas sobre a relação entre as variáveis. Identificar e avaliar a influência desses pontos ajuda a garantir a robustez do modelo e a confiabilidade das inferências feitas a partir dele.

Uma métrica comum para identificar pontos influentes é a distância de Cook, que combina tanto a magnitude do resíduo quanto a alavanca de uma observação para determinar sua influência no ajuste do modelo. Valores próximos de 0 indicam que a observação não é influente, maiores que 1 sugerem que a observação pode ser influente e entre 0 e 1 indicam que a observação pode ter alguma influência, mas não é necessariamente preocupante. A seguir estão essas distâncias representadas graficamente:

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# GRÁFICO - DISTÂNCIA DE COOK

df.fit5 %>% ggplot(aes(x=obs5,y=dcook5,ymin=0,ymax=dcook5)) + 
  geom_point() + 
  geom_linerange() +
  geom_hline(yintercept = corte.cook5, color="red", linetype="dashed") + 
  xlab("Observação") + 
  ylab("Distância de Cook") + 
  theme_bw()

```

Como nenhuma observação ultrapassa o ponto de corte estipulado, não precisamos fazer nenhuma alteração no modelo vigente.

## Normalidade

O pressuposto da normalidade requer que os erros do modelo sejam distribuídos de forma aproximadamente normal. Este pressuposto é crucial, especialmente quando se deseja realizar testes de hipóteses e construir intervalos de confiança para os coeficientes de regressão.

Um dos procedimentos para avaliar se os residuos são normalmente distribuídos é o teste Lilliefors (Kolmogorov-Smirnov), que é usado para verificar se uma amostra de dados segue uma distribuição normal.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# TESTE DE NORMALIDADE

#t1 <- ks.test(rst5,"pnorm")	#KS
t2 <- lillie.test(rst5)		  # Lilliefors
#t3 <- cvm.test(rst5)		      # Cramér-von Mises
#t4 <- shapiro.test(rst5)		  # Shapiro-Wilk
#t5 <- sf.test(rst5)		      # Shapiro-Francia
#t6 <- ad.test(rst5)		      # Anderson-Darling

# Tabela de resultados
testes <- c(#t1$method, 
  t2$method
  #, t3$method, t4$method, t5$method,t6$method
  )
estt <- as.numeric(c(#t1$statistic,
                     t2$statistic #,
                     #t3$statistic,
                     #t4$statistic,
                     #t5$statistic,
                     #t6$statistic
                     ))
valorp <- c(#t1$p.value, 
  t2$p.value
  #, t3$p.value, t4$p.value, t5$p.value,t6$p.value
  )
resultados <- cbind(estt, valorp)
rownames(resultados) <- testes
colnames(resultados) <- c("Estatística", "p")
round(resultados, digits = 4)



```

Considerando um nível de significância de 0.05, rejeitamos a hipótese de normalidade dos resíduos, já que o valor-p é 0.0346, menor que 0.05. Para cumprir o pressuposto e obter resíduos normalmente distribuídos, vamos usar a transformação de Box-Cox, que é uma técnica utilizada para transformar dados não normais em uma forma que siga uma distribuição aproximadamente normal, sendo útil na modelagem estatística e na análise de regressão para garantir a normalidade dos resíduos. Vamos escolher o valor de $\lambda$ para a transformação de forma que maximize a função de verossimilhança, mostrada graficamente abaixo:

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# TRANSFORMAÇÃO BOXCOX

transf_boxcox <- fit5 %>% boxcox(data=df)
lambda<- transf_boxcox$x[which.max(transf_boxcox$y)]

```

Assim, aplicamos a transformação para um $\lambda=0.1414141$ e realizamos o teste novamente, obtendo:

```{r, warning=FALSE, message=FALSE, echo=FALSE}
df <- df |>
  mutate(glucose_bc = (glucose^lambda - 1)/lambda) |>
  select(-c(glucose))

```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}

# Refazendo o modelo

modelo6 <- stats::lm(glucose_bc ~ ., data=df)
summary(modelo6)

opt_model_step_aic6 <- stepAIC(modelo6, direction="both") 
summary(opt_model_step_aic6)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

# TESTE DE NORMALIDADE

fit6 <- opt_model_step_aic6
rst6<- rstudent(fit6)		      # resíduos estudentizados

#t1 <- ks.test(rst5,"pnorm")	#KS
t2 <- lillie.test(rst6)		  # Lilliefors
#t3 <- cvm.test(rst5)		      # Cramér-von Mises
#t4 <- shapiro.test(rst5)		  # Shapiro-Wilk
#t5 <- sf.test(rst5)		      # Shapiro-Francia
#t6 <- ad.test(rst5)		      # Anderson-Darling

# Tabela de resultados
testes <- c(#t1$method, 
  t2$method
  #, t3$method, t4$method, t5$method,t6$method
  )
estt <- as.numeric(c(#t1$statistic,
                     t2$statistic #,
                     #t3$statistic,
                     #t4$statistic,
                     #t5$statistic,
                     #t6$statistic
                     ))
valorp <- c(#t1$p.value, 
  t2$p.value
  #, t3$p.value, t4$p.value, t5$p.value,t6$p.value
  )
resultados <- cbind(estt, valorp)
rownames(resultados) <- testes
colnames(resultados) <- c("Estatística", "p")
round(resultados, digits = 4)

```

Aqui obtemos um p-valor maior que 0.05, assim aceitamos a hipótese de que os resíduos são normalmente distribuídos.

## Homoscedasticidade

A homocedasticidade ocorre quando a variância dos erros (ou resíduos) é constante ao longo de todos os níveis das variáveis independentes. Quando esse pressuposto é satisfeito, dizemos que os erros são homocedásticos. Caso contrário, quando a variância dos erros varia, temos heterocedasticidade. A presença de heterocedasticidade pode levar a estimativas de coeficientes com variâncias subestimadas ou superestimadas, resultando em testes de significância e intervalos de confiança incorretos.

Para verificar a homocedasticidade, uma das ferramentas utilizadas é o Teste de Breusch-Pagan, que avalia a hipótese nula de que os erros têm variância constante. Um valor de p pequeno (tipicamente menor que 0,05) indica a presença de heterocedasticidade. Realizando esse teste em R obtemos o resultado:

```{r, warning=FALSE, message=FALSE}

bptest(fit6)			      # teste de Breusch-Pagan


```

Assim, aceitamos a hipótese nula de homocedasticidade.


## Erros não-correlacionados

O pressuposto de erros não-correlacionados significa que os resíduos (ou erros) do modelo devem ser independentes uns dos outros. Ou seja, o erro associado a uma observação não deve fornecer qualquer informação sobre o erro associado a outra observação. 

Para verificar a independência dos erros em um modelo de regressão, vamos usar o teste de Breusch-Godfrey, que examina a hipótese nula de que não há autocorrelação nos resíduos. A hipótese alternativa é que existe autocorrelação entre eles.

```{r, warning=FALSE, message=FALSE}

bgtest(fit6)			      # teste de Breusch-Godfrey

```
Nesse caso, como obtemos um p-valor maior que o nível de significância 0.05, aceitamos a hipótese de erros não correlacionados.

## Conclusão

Neste trabalho, abordamos a construção de um modelo de regressão para a variável glucose em um conjunto de dados coletados pelo Instituto Nacional de Diabetes e Doenças Digestivas e Renais. O estudo envolveu diversas variáveis preditoras como *pregnancies*, *bloodPressure*, *skinThickness*, *insulin*, *bmi*, *diabetesPedigreeFunction*, *ag*e e *outcome*. Através dessa análise, destacamos a importância de verificar e satisfazer os pressupostos fundamentais da regressão linear para garantir a validade e a precisão das inferências estatísticas.












